### Check the model in Overleaf documentation

library(fields)
library(ggplot2)
library(viridis)
library(plot3D)
library(fBasics)  # For vectorize columns of a matrix
library(MCMCpack)
library(mvtnorm)

set.seed(3019)

# dimension of the random field
q <- 3

#as.integer(readline(prompt="Enter the dimension of the random field (q): "))

N <- 2e2 # No. of spatial locations


#simulating N locations over [0,1]^2 in locations matrix

locations <- matrix(0, nrow = N, ncol = 2)

locations[, 1] <- runif(N)
locations[, 2] <- runif(N)


# Adding a mean term

X = cbind(1, locations)

#design matrix in concatenated form of dim (Np * 1) 
X_vec <- vec(t(X))  


#Number of features in the spatial model
p <- ncol(X)


# True value of regression matrix beta
true_beta <- matrix(c(-1.25, 2, 0.8, 0.35, -1.72, 2.5, 2.12, 0.45, -2.32), 
                    nrow = 3, byrow = TRUE)
  
  
#matrix(rnorm(p * q, mean = 0, sd = 2), nrow = p, ncol = q)  

#beta_mat is the kronecker product of the coefficient matrix across locations

beta_mat <- diag(N) %x% t(true_beta) 

# Fixed effect (I_n * B^T) X
mu_vec <- c(beta_mat %*% X_vec)  


# True value of componentwise var-cov matrix
#true_Sigma <- matrix(runif(q^2, 0.5, 3), nrow = q) # Random symmetric positive definite matrix
#true_Sigma <- sqrt(true_Sigma %*% t(true_Sigma))  # Ensuring positive definiteness

true_Sigma <- matrix(c(2,1,1,1,2,1,1,1,2), nrow = q, ncol =q, byrow = TRUE)

# true_phi

true_phi <- 0.2

# true_nu

true_nu <- 0.5

#true_r

true_r <- 0.8

# Calculates the Euclidean distance matrix

distmat <- rdist(locations) 

# Calculates the correlation matrix from matern kernel

K <- Matern(distmat, range = true_phi, smoothness = true_nu)

# Calculates the separable covariance matrix with nugget effect

Omega <- (true_r*K  + (1-true_r)*diag(N)) %x% true_Sigma

# Generating the response vector of dimension Nq*1

Y_vec <- mu_vec + c(t(chol(Omega)) %*% rnorm(N*q))

# Saving necessary parameters and data
save(N,  p, q, locations, X, Y_vec, true_beta, 
     true_Sigma, true_phi, true_nu, true_r, file = "separable_mgp_data.Rdata") 


# MCMC Set up
# Log-likelihood function for Gaussian Process regression using Cholesky decomposition

log_likelihood <- function(theta_list, Y_vec, locations) {
  
  beta <- theta_list[[1]]
  Sigma <- theta_list[[2]]
  phi <- theta_list[[3]]
  nu <- theta_list[[4]]
  r <- theta_list[[5]]
  
  N <- nrow(locations)
  
  X = cbind(1, locations)
  
  p <- ncol(X)
  
  #design matrix in concatenated form of dim (Np * 1) 
  X_vec <- vec(t(X))  
  
  distmat <- rdist(locations) 
  
  if(phi > 0 & nu > 0 & r > 0 & r < 1){
    
    K.tilde <- r*Matern(distmat, range = phi, smoothness = nu) + (1- r)*diag(N)
    
    chol.K.tilde <- chol(K.tilde)
    prec.K.tilde <- chol2inv(chol.K.tilde)
    
    q <- ncol(Sigma)
    
    chol.Sigma <- chol(Sigma)
    prec.Sigma <- chol2inv(chol.Sigma)
    
    prec <- prec.K.tilde %x% prec.Sigma
    
    # Calculates the separable covariance matrix with nugget effect
    
    resid <- Y_vec - (diag(N) %x% t(beta)) %*% X_vec
    
    like.out <- - q*sum(log(diag(chol.K.tilde))) - N*sum(log(diag(chol.Sigma))) 
    - 0.5*sum((prec %*% resid) * resid)
    
  }else{
    
    like.out <- -Inf
  }
  
  return(like.out)
}

# log-posterior of the model parameters

log_posterior_theta <- function(theta_list, Y_vec, locations){
  
  beta <- theta_list[[1]]
  Sigma <- theta_list[[2]]
  phi <- theta_list[[3]]
  nu <- theta_list[[4]]
  r <- theta_list[[5]]
  
  if(phi > 0 & nu > 0 & r > 0 & r < 1){
    
    #prior_specifications
    
    prior_phi <- dunif(phi, 0, 1, log = TRUE)
    prior_nu <- dlnorm(nu, meanlog = -1.2, sdlog = 1, log = TRUE)
    prior_Sigma <- log(diwish(Sigma, v =  q + 1, S = 100*diag(q)))
    prior_beta <- dmvnorm(as.vector(vec(t(beta))), mean = rep(0, p*q), 
                          sigma = 100*diag(p*q), log = TRUE)
    prior_r <- dunif(r, 0, 1, log = TRUE)
    
    return(log_likelihood(theta_list, Y_vec, locations) +
             prior_beta +
             prior_Sigma +
             prior_phi +
             prior_nu +
             prior_r)
    
  }else{
    return(-Inf)
  }
}


metropolis_hastings <- function(theta_init, niters, tuning_params) {
  
  
  n_params <- length(theta_init)
  theta_chain <- vector("list", length = n_params)
  
  for (iter in 1:niters){
    theta_chain[[1]][[iter]] <- matrix(0, nrow = p, ncol = q)
    theta_chain[[2]][[iter]] <- matrix(0, nrow = q, ncol = q)
    theta_chain[[3]][[iter]] <- 0
    theta_chain[[4]][[iter]] <- 0
    theta_chain[[5]][[iter]] <- 0
    
  }
  
  
  error_beta <- sqrt(tuning_params[1]) * rnorm(niters*p*q, 0, 1)
  error_phi <- rnorm(niters, 0, tuning_params[3])
  error_nu <- rnorm(niters, 0, tuning_params[4])
  error_r <- rnorm(niters, 0, tuning_params[5])
  
  
  accept <- numeric(n_params)
  
  # Initialize chain
  for (j in 1:n_params) {
    
    
    theta_chain[[j]][[1]] <- theta_init[[j]]
    
    # print(paste("Initialization Parameter:", j, "Initial value:", theta_chain[[j]][1])) 
    
  }
  
  # Run Metropolis-Hastings
  for (iter in 2:niters) {
    
    if(iter %% ((niters)/10) == 0) print(paste0(100*(iter/(niters)), "%"))
    
    # At this level current takes the theta_chain at (iter-1)
    current <- lapply(theta_chain, function(x) x[[iter - 1]])
    
    
    ############ Updating beta
    
    proposed_beta <- vec(theta_chain[[1]][[iter - 1]]) + error_beta[((iter-2)*p*q +1): ((iter-1)*p*q)]
    
    current[[1]] <- proposed_beta
    dim(current[[1]]) = c(p, q)
    
    # Compute log posterior for the proposed value
    
    log.r.beta <- log_posterior_theta(current, Y_vec, locations) - 
                  log_posterior_theta(lapply(theta_chain, 
                                      function(x) x[[iter - 1]]), Y_vec, locations)
    
    # Accept or reject
    if (log(runif(1)) < log.r.beta) {
      
      # suspected wrong step  
      theta_chain[[1]][[iter]] <- current[[1]]
      
      # accept overcounts
      accept[1] <- accept[1] + 1
      
    } else {
      
      theta_chain[[1]][[iter]] <- theta_chain[[1]][[iter - 1]]
      
    }
  
  ########### Updating Sigma
  
  proposed_Sigma <- riwish(v = q + 1 + tuning_params[2], 
                           tuning_params[2]*theta_chain[[2]][[iter - 1]]) 
  
  current[[2]] <- proposed_Sigma 
  
  # Compute log posterior for the proposed value of Sigma
  
  log.r.Sigma <- log_posterior_theta(current, Y_vec, locations) - 
                 log_posterior_theta(lapply(theta_chain, 
                                           function(x) x[[iter - 1]]), 
                                           Y_vec, locations) -
                 log(diwish(W = current[[2]], 
                           v = q + 1 + tuning_params[2], 
                           S = tuning_params[2]* theta_chain[[2]][[iter-1]])) +
                 log(diwish(W = theta_chain[[2]][[iter-1]],
                           v = q + 1 + tuning_params[2],
                           S = tuning_params[2] * current[[2]]))
  
  # Accept or reject
  if (log(runif(1)) < log.r.Sigma) {
    
    # suspected wrong step  
    theta_chain[[2]][[iter]] <- current[[2]]
    
    # accept overcounts
    accept[2] <- accept[2] + 1
    
  } else {
    
    theta_chain[[2]][[iter]] <- theta_chain[[2]][[iter - 1]]
    
  }
  
  
  ####### Updating phi
  proposed_phi <- theta_chain[[3]][[iter - 1]] + error_phi[iter]
  
  current[[3]] <- proposed_phi
  
  # Compute log posterior for the proposed value
  
  log.r.phi <- log_posterior_theta(current, Y_vec, locations) - 
                log_posterior_theta(lapply(theta_chain, 
                               function(x) x[[iter - 1]]), Y_vec, locations)
  
  # Accept or reject
  if (log(runif(1)) < log.r.phi) {
    
    # suspected wrong step  
    theta_chain[[3]][[iter]] <- current[[3]]
    
    # accept overcounts
    accept[3] <- accept[3] + 1
    
  } else {
    
    theta_chain[[3]][[iter]] <- theta_chain[[3]][[iter - 1]]
    
  }
  
  ####### Updating nu
  proposed_nu <- theta_chain[[3]][[iter - 1]] + error_nu[iter]
  
  current[[4]] <- proposed_nu
  
  # Compute log posterior for the proposed value
  
  log.r.nu <- log_posterior_theta(current, Y_vec, locations) - 
              log_posterior_theta(lapply(theta_chain, 
                               function(x) x[[iter - 1]]), Y_vec, locations)
  
  # Accept or reject
  if (log(runif(1)) < log.r.nu) {
    
    # suspected wrong step  
    theta_chain[[4]][[iter]] <- current[[4]]
    
    # accept overcounts
    accept[4] <- accept[4] + 1
    
  } else {
    
    theta_chain[[4]][[iter]] <- theta_chain[[4]][[iter - 1]]
    
  }
  
  
  ####### Updating r
  proposed_r <- theta_chain[[5]][[iter - 1]] + error_r[iter]
  
  current[[5]] <- proposed_r
  
  # Compute log posterior for the proposed value
  
  log.r.r <- log_posterior_theta(current, Y_vec, locations) - 
             log_posterior_theta(lapply(theta_chain, 
                               function(x) x[[iter - 1]]), Y_vec, locations)
  
  # Accept or reject
  if (log(runif(1)) < log.r.r) {
    
    # suspected wrong step  
    theta_chain[[5]][[iter]] <- current[[5]]
    
    # accept overcounts
    accept[5] <- accept[5] + 1
    
  } else {
    
    theta_chain[[5]][[iter]] <- theta_chain[[5]][[iter - 1]]
    
  }

}

print(paste("Acceptance = ", accept/niters))

return(theta_chain)
}

# Sample initial parameters
theta_init <- list("beta" = matrix(rep(0, p*q), nrow = p, ncol = q),
                   "Sigma" = diag(q),
                   "phi" = 0.5,
                   "nu" = 1,
                   "r" = 0.5)

# Number of iterations
niters <- 1e4

# Tuning parameters list

tuning_params <- c(3, 1, 0.05, 0.1, 0.02)

# Run Metropolis-Hastings algorithm
theta_chain <- metropolis_hastings(theta_init = theta_init, niters = niters, 
                                   tuning_params = tuning_params)
